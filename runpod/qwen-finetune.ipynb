{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a26b4a5-2fce-485c-abe7-8b817c6596af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -qdm (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting transformers==4.53.0\n",
      "  Downloading transformers-4.53.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting accelerate==1.8.1\n",
      "  Downloading accelerate-1.8.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting trl==0.19.0\n",
      "  Downloading trl-0.19.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting peft==0.10.0\n",
      "  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting filelock (from transformers==4.53.0)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers==4.53.0)\n",
      "  Downloading huggingface_hub-0.33.4-py3-none-any.whl.metadata (14 kB)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-cache-dir --force-reinstall \\\n",
    "    \"transformers==4.53.0\" \\\n",
    "    \"accelerate==1.8.1\" \\\n",
    "    \"trl==0.19.0\" \\\n",
    "    \"peft==0.10.0\" \\\n",
    "    datasets           # 버전 고정 X → trl 과 호환되는 최신 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc8b7e1-b5b2-4376-8403-5c352653e765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trl 0.19.0\n",
      "transformers 4.53.0\n",
      "accelerate 1.8.1\n"
     ]
    }
   ],
   "source": [
    "import trl, transformers, accelerate\n",
    "print(\"trl\", trl.__version__)\n",
    "print(\"transformers\", transformers.__version__)\n",
    "print(\"accelerate\", accelerate.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa03ab4-2d3f-40f1-8ef0-084c67d1077a",
   "metadata": {},
   "source": [
    "# 데이터 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b56ef3ab-2cae-407d-92d1-aef3a1a82b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3265\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/sogmgm/sogm_dataset/main/event_dataset_for_ft.jsonl'\n",
    "df = pd.read_json(url, lines=True, encoding='utf-8')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eacf429-6c3e-4902-9856-6936698d9468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3265 entries, 0 to 3264\n",
      "Data columns (total 15 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   doc_id            3265 non-null   int64 \n",
      " 1   doc_title         3265 non-null   object\n",
      " 2   text              3265 non-null   object\n",
      " 3   text_category     3265 non-null   object\n",
      " 4   event_quantity    3265 non-null   int64 \n",
      " 5   event_sentence1   3265 non-null   object\n",
      " 6   event_sentence2   3152 non-null   object\n",
      " 7   event_sentence3   938 non-null    object\n",
      " 8   event_sentence4   310 non-null    object\n",
      " 9   event_sentence5   127 non-null    object\n",
      " 10  event_sentence6   52 non-null     object\n",
      " 11  event_sentence7   24 non-null     object\n",
      " 12  event_sentence8   9 non-null      object\n",
      " 13  event_sentence9   3 non-null      object\n",
      " 14  event_sentence10  2 non-null      object\n",
      "dtypes: int64(2), object(13)\n",
      "memory usage: 382.7+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['부동산', '산업', '오피니언', '증권'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(df.info())\n",
    "df['text_category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9179ee9-c7be-4b92-8a5d-865310013362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 5 번째 데이터 샘플 =====\n",
      "문서 ID: 21943\n",
      "제목: 목동6단지 안전진단 통과…재건축 탄력\n",
      "카테고리: 부동산\n",
      "이벤트 개수: 3\n",
      "\n",
      "===== 본문 =====\n",
      "목동6단지가 서울 양천구 목동신시가지 2만6000여 가구 중 최초로 재건축 정밀안전진단을 조건부로 통과했다. 정밀안전진단은 재건축을 위한 첫 관문이다. 지난주 목동 재건축에서 가장 큰 걸림돌이었던 1~3단지 종상향(용적률 50% 증가) 문제도 해결돼 이 일대 재건축사업이 급물살을 탈 전망이다. 다만 이번 안전진단이 조건부 통과이고 향후 정비구역 지정, 조합 설립 및 각종 인가 등 숱한 단계가 남아 있어 본격적으로 재건축이 추진되기까지 상당한 시일이 소요될 전망이다.  31일 양천구청은 목동신시가지6단지 재건축을 위한 정밀안전진단 결과가 'D등급'이라고 목동6단지 재건축추진준비위원회에 통보했다. 재건축 안전진단의 경우 A~C등급은 유지·보수(재건축 불가), D등급은 조건부 재건축(공공기관 검증 필요), E등급은 재건축 확정 판정으로 분류된다. 목동6단지는 D등급을 받은 만큼 향후 6개월간 국토교통부 산하 한국시설안전공단의 적정성 검토를 거쳐 최종 안전진단 결과를 통보받는다.  이번에 목동6단지는 종합평가 결과 51.22점을 받아 조건부 재건축 점수 기준(30점 초과~55점 이하)을 충족했다. 지난해 10월 안전진단을 통과하지 못한 송파구 방이동 올림픽선수촌아파트(이하 올선)는 58.61점으로 재건축 불가 판정을 받았는데 이와 비교해보면 7점 넘게 낮은 점수를 받았다.    두 아파트 단지를 비교해 보면 목동6단지는 항목 4개 중 2개(주거 환경·비용 분석)에선 올선에 비해 점수가 높았지만, 나머지 2개 항목(건축 마감 및 설비노후도·구조안전성)은 점수가 낮았다. 다시 말해 목동은 주거 환경은 올선에 비해 좋지만 노후화돼 있고 구조적으로 안전하지 않다는 것이다. 신종섭 목동6단지 재건축추진준비위원장은 \"목동의 경우 아파트 아래에 철근 기둥만 박고, 철근 기둥을 받치는 지지대가 없어 지진이 멀리서 나면 사람들이 흔들림을 느낄 정도\"라고 설명했다. 다만 이와 관련해 '2년 더 먼저 지어진' 목동6단지가 정밀안전진단 중 가장 가중치(50%)가 높은 구조안전성 분야에서 올선보다 20점 넘게 낮은 점수(목동6단지 60.68점·올선 81.91점)를 받은 것을 두고 향후 논란의 가능성이 제기된다. 올선 재건축모임은 지난해 10월 통보된 안전진단 결과에 대해 강한 불신을 드러내고 있다.  이날 목동6단지가 정밀안전진단을 조건부로 통과하면서 목동 일대는 재건축 추진 기대감으로 들썩이는 분위기다. 특히 지난주 목동1~3단지 주민들의 숙원인 종상향(2종→3종·재건축 시 5100여 가구를 더 지을 수 있게 됨)이 서울시 심의를 통과해 사업성도 좋아졌다. 다만 공공기관의 적정성 검토가 남아 있다. 결과는 6개월 후에 나온다. 앞서 구로구 오류동 동부그린은 적정성 검토에서 1차 때 D등급을 받았는데 C등급으로 수정된 바 있다.  아울러 특별계획구역 지정도 문제다. 1~3단지는 특별계획구역에 잠정적으로 지정됐지만, 아직 4~14단지는 지정되지 못하고 있다. 특별계획구역이란 대규모 개발사업에 대해 서울시가 건축물 위치 등에 대한 세세한 지침을 주는 대신 종상향과 같은 혜택을 보는 지역이다. 서울시 관계자는 \"부동산 시장 과열 여부 등을 면밀히 살펴보면서 전체 목동 단지에 대한 특별계획구역 지정을 검토할 것\"이라며 시기를 조율할 것임을 시사했다.  [손동우 기자 / 나현준 기자]\n",
      "\n",
      "=====이벤트 문장들 =====\n",
      "이벤트 1: 목동6단지가 서울 양천구 목동신시가지 2만6000여 가구 중 최초로 재건축 정밀안전진단을 조건부로 통과했다.\n",
      "이벤트 2: 31일 양천구청은 목동신시가지6단지 재건축을 위한 정밀안전진단 결과가 'D등급'이라고 목동6단지 재건축추진준비위원회에 통보했다.\n",
      "이벤트 3: 앞서 구로구 오류동 동부그린은 적정성 검토에서 1차 때 D등급을 받았는데 C등급으로 수정된 바 있다.\n"
     ]
    }
   ],
   "source": [
    "# 데이터 출력\n",
    "n= 5 # n번째 문서\n",
    "print(f\"===== {n} 번째 데이터 샘플 =====\")\n",
    "print(f\"문서 ID: {df.iloc[n]['doc_id']}\")\n",
    "print(f\"제목: {df.iloc[n]['doc_title']}\")\n",
    "print(f\"카테고리: {df.iloc[n]['text_category']}\")\n",
    "print(f\"이벤트 개수: {df.iloc[n]['event_quantity']}\")\n",
    "\n",
    "print(\"\\n===== 본문 =====\")\n",
    "print(df.iloc[n]['text'])\n",
    "\n",
    "print(\"\\n=====이벤트 문장들 =====\")\n",
    "for i in range(1, 11):\n",
    "    event_col = f'event_sentence{i}'\n",
    "    if pd.notna(df.iloc[n][event_col]):\n",
    "        print(f\"이벤트 {i}: {df.iloc[n][event_col]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9fc51bd-4ac0-49c3-89a6-2a358d78cf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ===== 시스템 프롬프트  =====\n",
      "당신은 뉴스 텍스트를 분석하여 카테고리를 분류하고 주요 핵심 이벤트들을 추출하는 전문 분석 시스템입니다.\n",
      "주어진 텍스트를 분석하여 반드시 파이썬의 dictionary 형식으로 결과를 작성하십시오.\n",
      "큰 따옴표 사이에 다른 따옴표들을 적으려고 시도하지 마십시오. 이는 dictionary 파싱을 실패하게 하는 원인이 됩니다.\n",
      "아래 dictionary에서 각 value는 지시사항에 해당합니다. 지시사항을 그대로 적지 마시고, 해당 지시사항에 따라 적절한 value를 채워넣으십시오.\n",
      "\n",
      "분석 결과는 다음 형식으로 작성하십시오:\n",
      "\n",
      "답변:\n",
      "{\"category\": \"텍스트의 카테고리를 ['부동산', '산업', '오피니언', '증권'] 중 하나로 분류하여 작성하십시오\",\n",
      "\"event_count\": \"텍스트 내에서 발견된 핵심 이벤트의 개수를 정수로 작성하십시오(증감, 변화, 변동이 나오는 구절을 중점적으로 보세요.)\",\n",
      "\"events\": [\"텍스트에서 추출한 각 핵심 이벤트를 요약하는 문장들을 파이썬 문자열 리스트 형태로 작성하십시오. 원문의 핵심 내용을 그대로 유지하되 한 문장으로 간결하게 작성하십시오\"]}\n",
      "\n",
      "===== 사용자 프롬프트  =====\n",
      "다음 뉴스 텍스트를 분석해주세요:\n",
      "\n",
      "대통령 선거 후 수도권을 중심으로 한 부동산시장이 조금씩 바닥을 다지며 다시 상승을 준비하는 모습이 곳곳에서 포착되고 있다. 윤석열 대통령 당선인의 재건축·재개발 공약으로 서울에서 재건축 단지가 많은 지역 아파트값은 하락폭을 줄이고 있고, 일선 부동산 공인중개소에서는 매물을 거둬들이는 이가 점차 늘고 있는 것으로 나타났다...\n",
      "\n",
      "===== 어시스턴트 응답 =====\n",
      "{\"category\": \"부동산\",\"event_count\": 2,\"events\": [\"17일 한국부동산원에 따르면 서울의 3월 둘째 주(14일 기준) 아파트 매매가격지수는 전주 대비 0.02% 하락했다.\", \"경기 지역 역시 재건축 대상 아파트가 많은 고양(-0.02%→0%)이 보합으로 돌아섰고, 성남 분당은 전주와 같은 하락폭(-0.01%)을 기록했다.\"]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def create_structured_chat_data(df):\n",
    "    training_data = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        # 이벤트 문장들 수집\n",
    "        event_sentences = []\n",
    "        for i in range(1, 11):\n",
    "            event_col = f'event_sentence{i}'\n",
    "            if pd.notna(row[event_col]):\n",
    "                event_sentences.append(row[event_col])\n",
    "\n",
    "        # 구조화된 시스템 프롬프트\n",
    "        system_prompt = \"\"\"당신은 뉴스 텍스트를 분석하여 카테고리를 분류하고 주요 핵심 이벤트들을 추출하는 전문 분석 시스템입니다.\n",
    "주어진 텍스트를 분석하여 반드시 파이썬의 dictionary 형식으로 결과를 작성하십시오.\n",
    "큰 따옴표 사이에 다른 따옴표들을 적으려고 시도하지 마십시오. 이는 dictionary 파싱을 실패하게 하는 원인이 됩니다.\n",
    "아래 dictionary에서 각 value는 지시사항에 해당합니다. 지시사항을 그대로 적지 마시고, 해당 지시사항에 따라 적절한 value를 채워넣으십시오.\n",
    "\n",
    "분석 결과는 다음 형식으로 작성하십시오:\n",
    "\n",
    "답변:\n",
    "{\"category\": \"텍스트의 카테고리를 ['부동산', '산업', '오피니언', '증권'] 중 하나로 분류하여 작성하십시오\",\n",
    "\"event_count\": \"텍스트 내에서 발견된 핵심 이벤트의 개수를 정수로 작성하십시오(증감, 변화, 변동이 나오는 구절을 중점적으로 보세요.)\",\n",
    "\"events\": [\"텍스트에서 추출한 각 핵심 이벤트를 요약하는 문장들을 파이썬 문자열 리스트 형태로 작성하십시오. 원문의 핵심 내용을 그대로 유지하되 한 문장으로 간결하게 작성하십시오\"]}\"\"\"\n",
    "\n",
    "        # 사용자 프롬프트\n",
    "        user_prompt = f\"\"\"다음 뉴스 텍스트를 분석해주세요:\n",
    "\n",
    "{row['text']}\"\"\"\n",
    "\n",
    "        # 어시스턴트 응답 (JSON 형식)\n",
    "        events_list = [event.strip() for event in event_sentences]\n",
    "\n",
    "        assistant_response = f\"\"\"{{\"category\": \"{row['text_category']}\",\"event_count\": {row['event_quantity']},\"events\": {json.dumps(events_list, ensure_ascii=False)}}}\"\"\"\n",
    "\n",
    "        sample = {\n",
    "                      \"doc_id\": row['doc_id'],\n",
    "                      \"system_prompt\": system_prompt,\n",
    "                      \"user_prompt\": user_prompt,\n",
    "                      \"assistant\": assistant_response\n",
    "                  }\n",
    "\n",
    "        training_data.append(sample)\n",
    "\n",
    "    return training_data\n",
    "\n",
    "# 데이터 함수 실행\n",
    "training_data = create_structured_chat_data(df)\n",
    "\n",
    "# 첫 번째 샘플 확인\n",
    "print(\" ===== 시스템 프롬프트  =====\")\n",
    "print(training_data[0]['system_prompt'])\n",
    "print(\"\\n===== 사용자 프롬프트  =====\")\n",
    "print(training_data[0]['user_prompt'][:200] + \"...\")\n",
    "print(\"\\n===== 어시스턴트 응답 =====\")\n",
    "print(training_data[0]['assistant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28432406-34ec-47c7-8a0c-7a5580cfc02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ===== 포맷된 데이터 샘플  =====\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': '당신은 뉴스 텍스트를 분석하여 카테고리를 분류하고 주요 핵심 이벤트들을 추출하는 전문 분석 시스템입니다.\\n주어진 텍스트를 분석하여 반드시 파이썬의 dictionary 형식으로 결과를 작성하십시오.\\n큰 따옴표 사이에 다른 따옴표들을 적으려고 시도하지 마십시오. 이는 dictionary 파싱을 실패하게 하는 원인이 됩니다.\\n아래 dictionary에서 각 value는 지시사항에 해당합니다. 지시사항을 그대로 적지 마시고, 해당 지시사항에 따라 적절한 value를 채워넣으십시오.\\n\\n분석 결과는 다음 형식으로 작성하십시오:\\n\\n답변:\\n{\"category\": \"텍스트의 카테고리를 [\\'부동산\\', \\'산업\\', \\'오피니언\\', \\'증권\\'] 중 하나로 분류하여 작성하십시오\",\\n\"event_count\": \"텍스트 내에서 발견된 핵심 이벤트의 개수를 정수로 작성하십시오(증감, 변화, 변동이 나오는 구절을 중점적으로 보세요.)\",\\n\"events\": [\"텍스트에서 추출한 각 핵심 이벤트를 요약하는 문장들을 파이썬 문자열 리스트 형태로 작성하십시오. 원문의 핵심 내용을 그대로 유지하되 한 문장으로 간결하게 작성하십시오\"]}'},\n",
       "  {'role': 'user',\n",
       "   'content': '다음 뉴스 텍스트를 분석해주세요:\\n\\n대통령 선거 후 수도권을 중심으로 한 부동산시장이 조금씩 바닥을 다지며 다시 상승을 준비하는 모습이 곳곳에서 포착되고 있다. 윤석열 대통령 당선인의 재건축·재개발 공약으로 서울에서 재건축 단지가 많은 지역 아파트값은 하락폭을 줄이고 있고, 일선 부동산 공인중개소에서는 매물을 거둬들이는 이가 점차 늘고 있는 것으로 나타났다. 다만 전문가들은 아직 부동산과 관련해 구체적인 정책들이 나오지 않았기에 매도자들은 좀 더 지켜볼 필요가 있다는 반응이다.     17일 한국부동산원에 따르면 서울의 3월 둘째 주(14일 기준) 아파트 매매가격지수는 전주 대비 0.02% 하락했다. 8주 연속 떨어졌지만 지난주(-0.02%)에 비해 하락폭이 더 커지지는 않았다. 특히 재건축 단지가 많은 노원구(-0.02%→-0.01%)와 양천구(-0.01%→0%)는 하락폭이 감소했거나 보합으로 돌아섰고, 강남구(-0.01%→0%), 송파구(-0.01%→0%), 강동구(-0.03%→-0.02%) 등 강남권도 전주에 비해 회복되는 모습이었다.   경기 지역 역시 재건축 대상 아파트가 많은 고양(-0.02%→0%)이 보합으로 돌아섰고, 성남 분당은 전주와 같은 하락폭(-0.01%)을 기록했다. 부동산원은 이천(0.27%)은 정주 여건이 양호한 창전·안흥·증포동 위주로, 파주(0.08%)는 문산읍·야동동 역세권 위주로, 일산서구(0.03%)는 대화·일산동 구축 위주로 상승했다고 설명했다. 전국(-0.02%)과 수도권(-0.03%) 아파트값 변동률은 전주와 다르지 않았고, 지방은 -0.01%에서 0%로 보합으로 전환됐다.  재건축 아파트 단지가 많은 서울 지역 공인중개사들도 대선 후 변하고 있는 분위기를 전했다. 노원구 상계동 A공인중개사 대표는 \"기존에 매물을 내놨던 소유자 중 호가를 높이기 위해 일단 거둬들이는 이들이 나오고 있다\"며 \"대선 전에 비해 조금씩 분위기가 변하는 것 같다\"고 전했다. 강남구 압구정동 B공인중개사 대표는 \"재건축과 관련된 문의가 늘고 있는 것은 맞는 것 같다\"면서 바쁜 모습을 보이며 전화를 끊기도 했다.   목동 신시가지 7단지 아파트 전용면적 66㎡는 최근까지 호가가 20억원 선에 눌려 있다가 대선 이후 21억원대로 상승하기도 했다.   하지만 아직 실제 매수로 이어지지는 않고 관망하는 모습이 강해 보인다.   서울부동산정보광장에 따르면 17일 현재 서울시 3월 아파트 매매 거래량은 127건으로 작년 같은 기간 거래량(3762건)에 비해 3.3%에 불과하다.   부동산원 역시 \"서울은 규제 완화 기대감이 있는 재건축 단지나 한강변 인기 단지의 경우 매물이 소폭 감소하고 호가가 상승했지만 매수세로 이어지지는 않고 있다\"고 밝혔다.  부동산 전문가들은 아직 구체적인 규제 완화책이 나오지 않은 만큼 매도자들은 좀 더 관망하는 한편, 매수자들은 조금 서두를 필요가 있다고 말한다.   김제경 투미부동산컨설팅 소장은 \"재건축 아파트 매수와 관련한 문의가 최근 쏟아지고 있다\"며 \"매도자들의 경우 서두를 필요가 없지만 오는 6월 지방선거 전 또 한번 공약이 쏟아질 것을 감안하면 매수자들은 미리 움직이는 것이 좋다\"고 조언했다. 고준석 제이에듀투자자문 대표 역시 \"시장은 단기간에는 상승으로 돌아설 것으로 보이며 확실한 자금 계획이 있는 매수자들은 매수에 나서는 게 좋아 보인다\"고 설명했다.    특히 오는 7월 말 임대차3법이 시행된 지 만 2년이 되면서 제도 시행 후 2년의 전세계약갱신청구권을 사용한 이들이 기간 만료 전 전세시장에 다시 나오면 전세와 매매시장 모두를 자극할 수 있기 때문에 매수자들은 좀 더 먼저 움직일 필요가 있다는 말도 들린다.     [박준형 기자]'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"category\": \"부동산\",\"event_count\": 2,\"events\": [\"17일 한국부동산원에 따르면 서울의 3월 둘째 주(14일 기준) 아파트 매매가격지수는 전주 대비 0.02% 하락했다.\", \"경기 지역 역시 재건축 대상 아파트가 많은 고양(-0.02%→0%)이 보합으로 돌아섰고, 성남 분당은 전주와 같은 하락폭(-0.01%)을 기록했다.\"]}'}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_data(sample):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": sample[\"system_prompt\"],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": sample[\"user_prompt\"],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": str(sample[\"assistant\"])\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "# 전체 데이터에 format_data 적용\n",
    "formatted_data = [format_data(sample) for sample in training_data]\n",
    "\n",
    "# 확인\n",
    "print(\"\\n ===== 포맷된 데이터 샘플  =====\")\n",
    "formatted_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f90ee413-6159-44c2-b0f2-2cd930d97e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe18a109c54a4b4488a51b09fa6d2f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델과 토크나이저 로드 완료!\n",
      "Tokenizer vocab size: 151643\n",
      "Model parameters: 1,720,574,976\n"
     ]
    }
   ],
   "source": [
    "# 모델과 토크나이저 로드\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-1.7B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-1.7B\")\n",
    "\n",
    "# pad_token 설정 (필요한 경우)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"모델과 토크나이저 로드 완료!\")\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00d4dfa9-c21f-4b32-ba94-52f277122b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "템플릿 적용 완료: 3265개 샘플\n",
      "첫 번째 샘플 길이: 2684\n",
      "<|im_start|>system\n",
      "당신은 뉴스 텍스트를 분석하여 카테고리를 분류하고 주요 핵심 이벤트들을 추출하는 전문 분석 시스템입니다.\n",
      "주어진 텍스트를 분석하여 반드시 파이썬의 dictionary 형식으로 결과를 작성하십시오.\n",
      "큰 따옴표 사이에 다른 따옴표들을 적으려고 시도하지 마십시오. 이는 dictionary 파싱을 실패하게 하는 원인이 됩니다.\n",
      "아래 dictionary에서 각 value는 지시사항에 해당합니다. 지시사항을 그대로 적지 마시고, 해당 지시사항에 따라 적절한 value를 채워넣으십시오.\n",
      "\n",
      "분석 결과는 다음 형식으로 작성하십시오:\n",
      "\n",
      "답변:\n",
      "{\"category\": \"텍스트의 카테고리를 ['부동산', '산업', '오피니언', '증권'] 중 하나로 분류하여 작성하십시오\",\n",
      "\"event_count\": \"텍스트 내에서 발견된 핵심 이벤트의 개수를 정수로 작성하십시오(증감, 변화, 변동이 나오는 구절을 중점적으로 보세요.)\",\n",
      "\"events\": [\"텍스트에서 추출한 각 핵심 이벤트를 요약하는 문장들을 파이썬 문자열 리스트 형태로 작성하십시오. 원문의 핵심 내용을 그대로 유지하되 한 문장으로 간결하게 작성하십시오\"]}<|im_end|>\n",
      "<|im_start|>user\n",
      "다음 뉴스 텍스트를 분석해주세요:\n",
      "\n",
      "대통령 선거 후 수도권을 중심으로 한 부동산시장이 조금씩 바닥을 다지며 다시 상승을 준비하는 모습이 곳곳에서 포착되고 있다. 윤석열 대통령 당선인의 재건축·재개발 공약으로 서울에서 재건축 단지가 많은 지역 아파트값은 하락폭을 줄이고 있고, 일선 부동산 공인중개소에서는 매물을 거둬들이는 이가 점차 늘고 있는 것으로 나타났다. 다만 전문가들은 아직 부동산과 관련해 구체적인 정책들이 나오지 않았기에 매도자들은 좀 더 지켜볼 필요가 있다는 반응이다.     17일 한국부동산원에 따르면 서울의 3월 둘째 주(14일 기준) 아파트 매매가격지수는 전주 대비 0.02% 하락했다. 8주 연속 떨어졌지만 지난주(-0.02%)에 비해 하락폭이 더 커지지는 않았다. 특히 재건축 단지가 많은 노원구(-0.02%→-0.01%)와 양천구(-0.01%→0%)는 하락폭이 감소했거나 보합으로 돌아섰고, 강남구(-0.01%→0%), 송파구(-0.01%→0%), 강동구(-0.03%→-0.02%) 등 강남권도 전주에 비해 회복되는 모습이었다.   경기 지역 역시 재건축 대상 아파트가 많은 고양(-0.02%→0%)이 보합으로 돌아섰고, 성남 분당은 전주와 같은 하락폭(-0.01%)을 기록했다. 부동산원은 이천(0.27%)은 정주 여건이 양호한 창전·안흥·증포동 위주로, 파주(0.08%)는 문산읍·야동동 역세권 위주로, 일산서구(0.03%)는 대화·일산동 구축 위주로 상승했다고 설명했다. 전국(-0.02%)과 수도권(-0.03%) 아파트값 변동률은 전주와 다르지 않았고, 지방은 -0.01%에서 0%로 보합으로 전환됐다.  재건축 아파트 단지가 많은 서울 지역 공인중개사들도 대선 후 변하고 있는 분위기를 전했다. 노원구 상계동 A공인중개사 대표는 \"기존에 매물을 내놨던 소유자 중 호가를 높이기 위해 일단 거둬들이는 이들이 나오고 있다\"며 \"대선 전에 비해 조금씩 분위기가 변하는 것 같다\"고 전했다. 강남구 압구정동 B공인중개사 대표는 \"재건축과 관련된 문의가 늘고 있는 것은 맞는 것 같다\"면서 바쁜 모습을 보이며 전화를 끊기도 했다.   목동 신시가지 7단지 아파트 전용면적 66㎡는 최근까지 호가가 20억원 선에 눌려 있다가 대선 이후 21억원대로 상승하기도 했다.   하지만 아직 실제 매수로 이어지지는 않고 관망하는 모습이 강해 보인다.   서울부동산정보광장에 따르면 17일 현재 서울시 3월 아파트 매매 거래량은 127건으로 작년 같은 기간 거래량(3762건)에 비해 3.3%에 불과하다.   부동산원 역시 \"서울은 규제 완화 기대감이 있는 재건축 단지나 한강변 인기 단지의 경우 매물이 소폭 감소하고 호가가 상승했지만 매수세로 이어지지는 않고 있다\"고 밝혔다.  부동산 전문가들은 아직 구체적인 규제 완화책이 나오지 않은 만큼 매도자들은 좀 더 관망하는 한편, 매수자들은 조금 서두를 필요가 있다고 말한다.   김제경 투미부동산컨설팅 소장은 \"재건축 아파트 매수와 관련한 문의가 최근 쏟아지고 있다\"며 \"매도자들의 경우 서두를 필요가 없지만 오는 6월 지방선거 전 또 한번 공약이 쏟아질 것을 감안하면 매수자들은 미리 움직이는 것이 좋다\"고 조언했다. 고준석 제이에듀투자자문 대표 역시 \"시장은 단기간에는 상승으로 돌아설 것으로 보이며 확실한 자금 계획이 있는 매수자들은 매수에 나서는 게 좋아 보인다\"고 설명했다.    특히 오는 7월 말 임대차3법이 시행된 지 만 2년이 되면서 제도 시행 후 2년의 전세계약갱신청구권을 사용한 이들이 기간 만료 전 전세시장에 다시 나오면 전세와 매매시장 모두를 자극할 수 있기 때문에 매수자들은 좀 더 먼저 움직일 필요가 있다는 말도 들린다.     [박준형 기자]<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "{\"category\": \"부동산\",\"event_count\": 2,\"events\": [\"17일 한국부동산원에 따르면 서울의 3월 둘째 주(14일 기준) 아파트 매매가격지수는 전주 대비 0.02% 하락했다.\", \"경기 지역 역시 재건축 대상 아파트가 많은 고양(-0.02%→0%)이 보합으로 돌아섰고, 성남 분당은 전주와 같은 하락폭(-0.01%)을 기록했다.\"]}<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 전체 데이터에 템플릿 적용\n",
    "texts = []\n",
    "for sample in formatted_data:\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        sample[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt= False,\n",
    "        chat_template_kwargs= {\"enable_thinking\": False}\n",
    "\n",
    "    )\n",
    "    texts.append(text)\n",
    "\n",
    "print(f\"템플릿 적용 완료: {len(texts)}개 샘플\")\n",
    "print(f\"첫 번째 샘플 길이: {len(texts[0])}\")\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1faa203-7590-4592-a961-30b5fb272e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f656d43c-6c4a-4900-bc5b-3d1b298da384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453ff859d4d648ac9894c5df9ca30ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3265 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토크나이징 완료: 3265개 샘플\n"
     ]
    }
   ],
   "source": [
    "def simple_tokenize_function(examples):\n",
    "    result = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=8192,\n",
    "        padding=False,\n",
    "        return_overflowing_tokens=False,\n",
    "    )\n",
    "\n",
    "    # labels 초기화\n",
    "    result[\"labels\"] = []\n",
    "\n",
    "    for i, text in enumerate(examples[\"text\"]):\n",
    "        input_ids = result[\"input_ids\"][i]\n",
    "        labels = [-100] * len(input_ids)\n",
    "\n",
    "        # <|im_start|>assistant\\n 토큰들 찾기 (전체 시퀀스)\n",
    "        assistant_tokens = tokenizer.encode(\"<|im_start|>assistant\\n\", add_special_tokens=False)\n",
    "\n",
    "        # assistant 시작 위치 찾기\n",
    "        for j in range(len(input_ids) - len(assistant_tokens) + 1):\n",
    "            if input_ids[j:j+len(assistant_tokens)] == assistant_tokens:\n",
    "                # assistant\\n 다음부터 학습 시작\n",
    "                start_idx = j + len(assistant_tokens)\n",
    "\n",
    "                # <|im_end|> 토큰 찾기\n",
    "                im_end_token = tokenizer.encode(\"<|im_end|>\", add_special_tokens=False)[0]\n",
    "                for k in range(start_idx, len(input_ids)):\n",
    "                    if input_ids[k] == im_end_token:\n",
    "                        end_idx = k\n",
    "                        break\n",
    "                else:\n",
    "                    end_idx = len(input_ids)\n",
    "\n",
    "                # assistant 답변 부분만 학습하도록 설정\n",
    "                labels[start_idx:end_idx] = input_ids[start_idx:end_idx]\n",
    "                break\n",
    "\n",
    "        result[\"labels\"].append(labels)\n",
    "\n",
    "    return result\n",
    "\n",
    "# 다시 토크나이징\n",
    "\n",
    "# texts 리스트를 dataset으로 변환\n",
    "dataset = Dataset.from_dict({\"text\": texts})\n",
    "tokenized_dataset = dataset.map(simple_tokenize_function, batched=True)\n",
    "print(f\"토크나이징 완료: {len(tokenized_dataset)}개 샘플\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c0a38ad-aa56-4e29-8616-96e0a870a1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 토크나이징 후 데이터셋 정보 =====\n",
      "Dataset({\n",
      "    features: ['text', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 3265\n",
      "})\n",
      "컬럼: ['text', 'input_ids', 'attention_mask', 'labels']\n",
      "첫 번째 샘플 input_ids 길이: 1685\n",
      "토큰 길이 - 최소: 619, 최대: 6885, 평균: 1394.5\n"
     ]
    }
   ],
   "source": [
    "# 토크나이징 후 데이터 확인\n",
    "print(\"===== 토크나이징 후 데이터셋 정보 =====\")\n",
    "print(tokenized_dataset)\n",
    "print(f\"컬럼: {tokenized_dataset.column_names}\")\n",
    "print(f\"첫 번째 샘플 input_ids 길이: {len(tokenized_dataset[0]['input_ids'])}\")\n",
    "\n",
    "# 길이 분포 확인\n",
    "lengths = [len(sample['input_ids']) for sample in tokenized_dataset]\n",
    "print(f\"토큰 길이 - 최소: {min(lengths)}, 최대: {max(lengths)}, 평균: {sum(lengths)/len(lengths):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee3f82a1-c504-4421-a4c7-415d4793e3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플 하나를 자세히 분석\n",
      "============================================================\n",
      "1. text (원본 텍스트)\n",
      "   문자 수: 2,684 문자\n",
      "   내용:\n",
      "   <|im_start|>system\n",
      "당신은 뉴스 텍스트를 분석하여 카테고리를 분류하고 주요 핵심 이벤트들을 추출하는 전문 분석 시스템입니다.\n",
      "주어진 텍스트를 분석하여 반드시 파이썬의 dictionary 형식으로 결과를 작성하십시오.\n",
      "큰 따옴표 사이에 다른 따옴표들을 적으려고 시도하지 마십시오. 이는 dictionary 파싱을 실패하게 하는 원인이 됩니다.\n",
      "아...\n",
      "   ...\", \"경기 지역 역시 재건축 대상 아파트가 많은 고양(-0.02%→0%)이 보합으로 돌아섰고, 성남 분당은 전주와 같은 하락폭(-0.01%)을 기록했다.\"]}<|im_end|>\n",
      "\n",
      "============================================================\n",
      "2. input_ids (토큰 ID 숫자들)\n",
      "   토큰 개수: 1685 개\n",
      "   시작 부분: [151644, 8948, 198, 64795, 82528, 33704, 5140, 231, 112, 24897, 10764, 44104, 53189, 18411, 128618]\n",
      "   끝 부분: [135257, 4080, 15, 13, 15, 16, 11334, 17877, 54116, 49664, 128836, 1189, 13989, 151645, 198]\n",
      "============================================================\n",
      "3. attention_mask (어느 토큰에 집중할지)\n",
      "   길이: 1685 (input_ids와 같아야 함)\n",
      "   시작: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] (모두 1이면 정상)\n",
      "   끝: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "   모든 값이 1? True\n",
      "   → 1=실제토큰, 0=패딩토큰 (현재는 패딩 없음)\n",
      "============================================================\n",
      "4. labels (모델이 학습할 정답)\n",
      "   길이: 1685 (input_ids와 같아야 함)\n",
      "   현재 input_ids와 동일? False\n",
      "   시작: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "   끝: [16, 11334, 17877, 54116, 49664, 128836, 1189, 13989, -100, -100]\n",
      "============================================================\n",
      "올바른 설정\n",
      "   -100인 부분은 학습 안 함 (사용자 질문)\n",
      "   실제 숫자인 부분만 학습 (assistant 답변)\n"
     ]
    }
   ],
   "source": [
    "# 첫 번째 샘플 선택\n",
    "sample = tokenized_dataset[0]\n",
    "print(\"샘플 하나를 자세히 분석\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"1. text (원본 텍스트)\")\n",
    "print(f\"   문자 수: {len(sample['text']):,} 문자\")\n",
    "print(f\"   내용:\")\n",
    "print(f\"   {sample['text'][:200]}...\")\n",
    "print(f\"   ...{sample['text'][-100:]}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"2. input_ids (토큰 ID 숫자들)\")\n",
    "print(f\"   토큰 개수: {len(sample['input_ids'])} 개\")\n",
    "print(f\"   시작 부분: {sample['input_ids'][:15]}\")\n",
    "print(f\"   끝 부분: {sample['input_ids'][-15:]}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"3. attention_mask (어느 토큰에 집중할지)\")\n",
    "print(f\"   길이: {len(sample['attention_mask'])} (input_ids와 같아야 함)\")\n",
    "print(f\"   시작: {sample['attention_mask'][:10]} (모두 1이면 정상)\")\n",
    "print(f\"   끝: {sample['attention_mask'][-10:]}\")\n",
    "print(f\"   모든 값이 1? {all(x == 1 for x in sample['attention_mask'])}\")\n",
    "print(\"   → 1=실제토큰, 0=패딩토큰 (현재는 패딩 없음)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"4. labels (모델이 학습할 정답)\")\n",
    "print(f\"   길이: {len(sample['labels'])} (input_ids와 같아야 함)\")\n",
    "print(f\"   현재 input_ids와 동일? {sample['input_ids'] == sample['labels']}\")\n",
    "print(f\"   시작: {sample['labels'][:10]}\")\n",
    "print(f\"   끝: {sample['labels'][-10:]}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if sample['input_ids'] == sample['labels']:\n",
    "    print(\"문제 발견!\")\n",
    "    print(\"   현재는 사용자 질문까지 학습하고 있어요!\")\n",
    "    print(\"   assistant 답변 부분만 학습해야 합니다.\")\n",
    "else:\n",
    "    print(\"올바른 설정\")\n",
    "    print(\"   -100인 부분은 학습 안 함 (사용자 질문)\")\n",
    "    print(\"   실제 숫자인 부분만 학습 (assistant 답변)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3331653-de05-4e97-9f2d-e491cfbb34aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d29fa90d-20ee-4cb4-9c1d-e6b1e446f6ea",
   "metadata": {},
   "source": [
    "# Train! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efa4d3b9-e6eb-4524-9b4b-51e7f5ee4c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74124841ad1c44ab96ebf1597ea829bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/3265 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "\n",
    "\n",
    "# LoRA 설정\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# 모델에 LoRA 적용\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "\n",
    "\n",
    "# 3) SFTConfig  ← 여기서 max_seq_length, packing 지정\n",
    "sft_cfg = SFTConfig(\n",
    "    output_dir=\"./qwen3-lora-ft\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    optim=\"adamw_torch\",\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    logging_steps=50,\n",
    "    save_steps=50,\n",
    "    save_total_limit=3,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    weight_decay=0.01,\n",
    "    max_seq_length = 8192,\n",
    "    packing        = False,\n",
    ")\n",
    "\n",
    "\n",
    "# 5) SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model             = model,\n",
    "    args              = sft_cfg,           # ← SFTConfig 객체\n",
    "    train_dataset     = tokenized_dataset,\n",
    "    eval_dataset      = None,\n",
    "    processing_class  = tokenizer,         # ← tokenizer 자리\n",
    ")\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "trainer.train_dataset = trainer.train_dataset.shuffle(seed=42)\n",
    "trainer.train()\n",
    "\n",
    "저장\n",
    "trainer.model.save_pretrained(\"./qwen3-lora-ft-final\")\n",
    "print(\"훈련 완료! 모델 저장됨.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aaa0d266-5853-404e-b5c7-65b62f1b6ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python : /usr/bin/python\n",
      "trl     : 0.19.0\n",
      "trl path: /usr/local/lib/python3.10/dist-packages/trl\n",
      "SFTTrainer sig: (self, model: Union[str, torch.nn.modules.module.Module, transformers.modeling_utils.PreTrainedModel], args: Union[trl.trainer.sft_config.SFTConfig, transformers.training_args.TrainingArguments, NoneType] = None, data_collator: Optional[transformers.data.data_collator.DataCollator] = None, train_dataset: Union[datasets.arrow_dataset.Dataset, datasets.iterable_dataset.IterableDataset, NoneType] = None, eval_dataset: Union[datasets.arrow_dataset.Dataset, dict[str, datasets.arrow_dataset.Dataset], NoneType] = None, processing_class: Union[transformers.tokenization_utils_base.PreTrainedTokenizerBase, transformers.image_processing_utils.BaseImageProcessor, transformers.feature_extraction_utils.FeatureExtractionMixin, transformers.processing_utils.ProcessorMixin, NoneType] = None, compute_loss_func: Optional[Callable] = None, compute_metrics: Optional[Callable[[transformers.trainer_utils.EvalPrediction], dict]] = None, callbacks: Optional[list[transformers.trainer_callback.TrainerCallback]] = None, optimizers: tuple[typing.Optional[torch.optim.optimizer.Optimizer], typing.Optional[torch.optim.lr_scheduler.LambdaLR]] = (None, None), optimizer_cls_and_kwargs: Optional[tuple[type[torch.optim.optimizer.Optimizer], dict[str, Any]]] = None, preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None, peft_config: Optional[ForwardRef('PeftConfig')] = None, formatting_func: Optional[Callable[[dict], str]] = None)\n"
     ]
    }
   ],
   "source": [
    "import sys, trl, inspect, importlib, pkg_resources, pathlib\n",
    "print(\"Python :\", sys.executable)\n",
    "print(\"trl     :\", trl.__version__)\n",
    "print(\"trl path:\", pathlib.Path(trl.__file__).parent)\n",
    "print(\"SFTTrainer sig:\", inspect.signature(trl.SFTTrainer.__init__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "600f0d4f-1b1b-44db-8ff3-7a16bead290a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: trl 0.19.0\n",
      "Uninstalling trl-0.19.0:\n",
      "  Successfully uninstalled trl-0.19.0\n",
      "Found existing installation: transformers 4.53.0\n",
      "Uninstalling transformers-4.53.0:\n",
      "  Successfully uninstalled transformers-4.53.0\n",
      "Found existing installation: accelerate 1.8.1\n",
      "Uninstalling accelerate-1.8.1:\n",
      "  Successfully uninstalled accelerate-1.8.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f664bc72-10f7-4bcd-95c9-dd8df95421b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(\"./qwen3-lora-ft-final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5a527f-6e49-43c4-9d07-47f0700b92d6",
   "metadata": {},
   "source": [
    "# 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80182fc1-3b0f-4e35-9b58-e4028ac9f213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b164d4c74c1b46aca1859318cd2d2b0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b6979bea82467c9767fb3d1c06946a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 베이스 모델 출력 -----\n",
      " {\"category\": \"산업\", \"event_count\": 4, \"events\": [\"6·27 대출 규제 시행 후 도봉구 아파트 가격 상승세 유지\", \"도봉구 실거래가 회복 조짐 나타나고 있다\", \"금관구 거래량 감소 및 가격 상승세 하락\", \"부동산업계 전문가가 외곽 지역 시장 흐름의 불확실성 강조\"]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 로라 모델 출력 -----\n",
      " {\"category\": \"부동산\",\"event_count\": 2,\"events\": [\"서울부동산정보광장에 따르면 도봉구 아파트 거래량은 5월 155건, 6월 289건으로 늘었지만, 7월 들어서는 20일 기준 51건에 머물고 있다.\", \"도봉구의 한 중개업소 대표는 \\\"이미 작년에 바닥을 찍었다는 인식이 확산되면서 실수요자 문의가 늘고 있고, 호가도 빠르게 회복되고 있다\\\"며 \\\"GTX-C, 창동 차량기지 개발 등 향후 호재가 여전한 만큼, 지금은 회복의 시작일 뿐이라는 얘기도 나온다\\\"고 했다.\"]}\n",
      "\n",
      "===== 1번째 비교 테스트 끝 =====\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 베이스 모델 출력 -----\n",
      " 답변:\n",
      "{\"category\": \"산업\", \"event_count\": 4, \"events\": [\"한일시멘트가 자회사 한일현대시멘트를 흡수 합병 결정\", \"시멘트 산업의 경영상황 악화\", \"업계 전반의 대응은 신중한 모습\", \"시멘트 제조 시 온실가스 배출량 증가로 탄소중립 목표 설정\"]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 로라 모델 출력 -----\n",
      " {\"category\": \"산업\",\"event_count\": 2,\"events\": [\"20일 관련 업계에 따르면 한일시멘트는 최근 이사회를 열고 77.78% 지분을 보유한 한일현대시멘트를 오는 11월 1일부로 흡수 합병하기로 의결했다.\", \"이미 시멘트 업계의 경영상황은 악화일로다.\"]}\n",
      "\n",
      "===== 2번째 비교 테스트 끝 =====\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 베이스 모델 출력 -----\n",
      " 답변:\n",
      "{\"category\": \"산업\", \"event_count\": 10, \"events\": [\"인공지능(AI) 분야 경쟁이 '매그니피센트 세븐' 기업 주가에 심화되며, 주가 차별화가 뚜렷해졌습니다.\", \"Amazon, Alphabet, Apple, Meta, Microsoft, NVIDIA, Tesla의 주가가 AI 성과에 따라 크게 갈리고 있습니다.\", \"S&P 500 지수에서 7개 기업이 차지하는 비중은 35%에 달하며, 이들 기업의 2분기 순이익이 전년 동기 대비 14% 증가했습니다.\", \"Alphabet과 Tesla는 주가가 20% 이상 상승했으나, Apple과 Meta는 16% 하락했습니다.\", \"Amazon은 관세 영향으로 전자상거래 부문에 타격을 받았지만, 주가가 3% 올랐습니다.\", \"Alphabet과 Tesla는 23일 분기 실적 발표를, Meta, Microsoft, Apple는 다음 주에 발표할 예정입니다.\", \"이바나 델레브스카는 7개 기업이 AI 분야 선봉에 있는 점을 강조하며, 펀더멘털 차이가 뚜렷하다고 평가했습니다.\", \"Harry Ferguson의 매니징 파트너인 제이미 콕스는 테크 대기업 간 실적 차이로 주가 흐름이 차별화된 점을 지적했다.\"}\n",
      "----- 로라 모델 출력 -----\n",
      " {\"category\": \"증권\",\"event_count\": 2,\"events\": [\"올해 들어 엔비디아, 메타, MS의 주가는 약 20% 이상 올랐으나, 이와 대조적으로 애플은 16% 하락했고 알파벳도 2% 내렸고 테슬라는 18%가 빠졌다.\", \"AI 스타트업 '앤스로픽'에 투자한 아마존은 관세 등의 영향으로 전자상거래 부문 사업에 타격이 상당히 큰 가운데서도 3% 올랐다.\"]}\n",
      "\n",
      "===== 3번째 비교 테스트 끝 =====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "# 1. 토크나이저와  모델 로드\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-1.7B\")\n",
    "\n",
    "base= AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen3-1.7B\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ").eval()                     \n",
    "\n",
    "\n",
    "lora = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen3-1.7B\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "lora = PeftModel.from_pretrained(\n",
    "    lora,\n",
    "    \"./qwen3-lora-ft-final\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\"\n",
    ").eval()\n",
    "\n",
    "# 3. 시스템 프롬프트 \n",
    "system_prompt = \"\"\"당신은 뉴스 텍스트를 분석하여 카테고리를 분류하고 주요 핵심 이벤트들을 추출하는 전문 분석 시스템입니다.\n",
    "주어진 텍스트를 분석하여 반드시 파이썬의 dictionary 형식으로 결과를 작성하십시오.\n",
    "큰 따옴표 사이에 다른 따옴표들을 적으려고 시도하지 마십시오. 이는 dictionary 파싱을 실패하게 하는 원인이 됩니다.\n",
    "아래 dictionary에서 각 value는 지시사항에 해당합니다. 지시사항을 그대로 적지 마시고, 해당 지시사항에 따라 적절한 value를 채워넣으십시오.\n",
    "\n",
    "분석 결과는 다음 형식으로 작성하십시오:\n",
    "\n",
    "답변:\n",
    "{\"category\": \"텍스트의 카테고리를 ['부동산', '산업', '오피니언', '증권'] 중 하나로 분류하여 작성하십시오\",\n",
    "\"event_count\": \"텍스트 내에서 발견된 핵심 이벤트의 개수를 정수로 작성하십시오(증감, 변화, 변동이 나오는 구절을 중점적으로 보세요.)\",\n",
    "\"events\": [\"텍스트에서 추출한 각 핵심 이벤트를 요약하는 문장들을 파이썬 문자열 리스트 형태로 작성하십시오. 원문의 핵심 내용을 그대로 유지하되 한 문장으로 간결하게 작성하십시오\"]}\"\"\"\n",
    "\n",
    "# 4. 테스트 데이터 3개 \n",
    "test_texts = [\n",
    "    # 1. 부동산 카테고리\n",
    "    \"\"\" 최대 수혜지역이라고요? 분위기 좀 달라지긴 했지만, 그게 꼭 6·27 대출 규제 때문만은 아니죠.\"\n",
    "\n",
    "지난 19일 서울 도봉구 창동역 인근 한 공인중개사는 최근 시장 분위기를 묻는 질문에 이같이 말했다. 도봉구는 창동 차량기지 복합개발, GTX-C 등 대형 교통 호재에 힘입어 올해 들어 아파트값이 상승세로 전환됐고, 일부 단지에서는 실거래가 회복 조짐도 나타나고 있다. 반면 6·27대책 발표 직후 상승세 강했던 금관구(금천·관악·구로)는 매수·매도인 모두 지켜보기에 들어간 모습이다.\n",
    "\n",
    "■도봉구 상승세…실거래가 회복 조짐\n",
    "\n",
    "6·27 대출 규제 시행 3주차에 접어들면서 서울 외곽 지역을 중심으로 시장의 온도차가 뚜렷해지고 있다. 한국부동산원 주간 아파트가격 동향에 따르면 7월 2주(15일 기준) 서울 아파트값은 0.19% 올라 전주(0.29%)보다 오름폭이 둔화됐다. 다만 도봉구(0.05%→0.06%)와 중구(0.16%→0.18%)는 상승폭이 소폭 확대됐다. 도봉구의 한 중개업소 관계자는 \"급매가 거의 소진된 상태에서 전고점에 근접한 거래도 하나둘 나타나고 있다\"며 \"전용 84㎡, 165㎡ 등 다양한 평형에 실수요자와 일부 투자자의 문의가 이어지고 있다\"고 전했다. 이어 \"6·27 대책 이후 도봉구처럼 상대적으로 대출 여건이 유지되는 중저가 지역에 관심이 모이고 있는 건 사실이지만, 지금 시장을 움직이는 건 결국 개발 기대감에 기반한 유동성 있는 수요자들\"이라고 설명했다.\n",
    "\n",
    "서울부동산정보광장에 따르면 도봉구 아파트 거래량은 5월 155건, 6월 289건으로 늘었지만, 7월 들어서는 20일 기준 51건에 머물고 있다. 규제 여파로 거래량은 다소 숨고르기에 들어간 양상이지만 가격 상승세는 계속될 것으로 전망됐다.\n",
    "\n",
    "도봉구의 또 다른 중개업소 대표는 \"이미 작년에 바닥을 찍었다는 인식이 확산되면서 실수요자 문의가 늘고 있고, 호가도 빠르게 회복되고 있다\"며 \"GTX-C, 창동 차량기지 개발 등 향후 호재가 여전한 만큼, 지금은 회복의 시작일 뿐이라는 얘기도 나온다\"고 했다.\n",
    "\n",
    "■금관구 숨고르기…거래량 감소 뚜렷\n",
    "\n",
    "반면 6·27 대책 직후 풍선효과 기대감이 부각됐던 '금관구'는 오름세가 한풀 꺾인 분위기다. 같은 기간 금천구는 0.09%→0.07%, 관악구는 0.19%→0.15%, 구로구는 0.18%→0.12%로 상승폭이 일제히 축소됐다.\n",
    "\n",
    "금천구 시흥동의 한 중개업소 관계자는 \"호가는 잠깐 반짝했지만 지금은 문의가 다시 줄어든 상황\"이라며 \"갭투자자든 실수요자든 모두 관망 중\"이라고 말했다. 관악구의 한 공인중개사도 \"전세보증금이 낮은 구축 소형 위주로 간헐적인 갭투자 수요는 있었지만, 지금은 전세금도 안 오르고 매수도 끊긴 상태\"라고 분위기를 전했다.\n",
    "\n",
    "서울부동산정보광장에 따르면 금천구 아파트 거래량은 5월 74건에서 6월 135건으로 증가한 뒤, 7월(20일 기준)에는 25건에 그쳤다. 관악구는 같은 기간 222건→294건→54건, 구로구는 678건→462건→84건으로 줄며 거래 감소세가 뚜렷하다.\n",
    "\n",
    "개봉동의 중개업소 관계자는 \"급매라고는 해도 1000만~2000만원 낮춘 수준인데, 그런 물건도 상반기에 이미 빠졌다\"며 \"매수자는 관망세, 매도자는 버티기에 들어간 상황\"이라고 전했다. 결론적으로 매물 자체가 줄어들면서 거래량도 감소한 위축된 시장 흐름을 보이고 있어, 풍선효과가 나타난다고 보기는 어렵다는 평가다.\n",
    "\n",
    "부동산업계 한 전문가는 \"서울 외곽 지역은 규제 변화에 민감하게 반응하는 편이지만, 신축 비중이나 입지 여건, 실수요자의 선호도에 따라 시장 흐름은 엇갈릴 수밖에 없다\"며 \"현재는 거래 절벽에 가까운 상태로 '풍선효과' 역시 일부 지역에 국한된 단기 흐름에 그칠 가능성이 높다\"고 진단했다.\",\n",
    "\"\"\",    \n",
    "    # 2. 산업 카테고리\n",
    "    \"\"\"\n",
    "    한일시멘트가 자회사 한일현대시멘트 흡수 합병을 통해 경영효율화에 나서면서 시멘트업계에 구조·사업개편 가능성이 제기된다. 국내 시멘트 산업이 '저성장 고비용'의 구조적 위험에 빠진데 따른 영향이라는 분석이다. 다만 한일시멘트를 제외한 주요 기업들은 우선 신중모드로 접근할 전망이다.\n",
    "\n",
    "20일 관련 업계에 따르면 한일시멘트는 최근 이사회를 열고 77.78% 지분을 보유한 한일현대시멘트를 오는 11월 1일부로 흡수 합병하기로 의결했다. 양사는 2017년부터 단계적으로 통합 작업을 진행해왔다. 이번 합병으로 상장사 이중 구조를 해소하고 중복투자·외부비용 절감을 통한 경영 효율화에 나선다는 방침이다. 한일시멘트 관계자는 \"경기가 부침이 있는 만큼 설비 등 인프라 활용해 탄력적 대응이 가능할 것\"이라고 말했다.\n",
    "\n",
    "이미 시멘트 업계의 경영상황은 악화일로다. 국토교통부에 따르면 지난 5월 건설착공면적은 전월 대비 26.9% 감소한 620만㎡ 수준에 그쳤다. 착공면적은 통상 건설경기의 선행지표로 여겨진다. 시멘트 출하량도 급감했다. 올해 1·4분기 시멘트 내수는 전년 동기 대비 21.8% 줄어든 812만t으로, 최근 5년 내 최저치를 기록했다. 2023년 1·4분기 1201만t 대비 2년 새 32.4%가 줄어든 셈이다.\n",
    "\n",
    "그럼에도 업계 전반의 대응은 신중한 모습이다. 한일을 제외한 주요 업체들은 합병 혹은 조직개편 계획 여부 등은 없다는 입장이다. 쌍용C&E 관계자는 \"경기가 좋지 않지만 수출 덕분에 그나마 나은 상황으로 당분간 특별한 계획 없다\"고 말했다. 성신양회, 한라시멘트 관계자 역시 \"거래처 및 비용 절감에 집중하고 있을 뿐, 지배구조 개편 등은 고려하고 있지 않다\"고 했다.\n",
    "\n",
    "업계는 현재 위기를 수요 부진과 고정비 부담의 이중고로 보고 있다. 시멘트 산업은 설비 비중이 높아 수요가 줄어도 일정 수준의 가동과 인력 유지를 지속해야 하기 때문이다. 게다가 최근 몇 년간 산업용 전기요금 인상, 탄소배출권 비용 증가, 폐기물 처리단가 상승 등 외부 비용도 커졌다.\n",
    "\n",
    "이 가운데 업계는 환경·사회·지배구조(ESG) 전환에도 동시에 대응 중이다. 시멘트 제조 시 온실가스 배출량이 많다는 점에서, 탄소중립 압력이 거세져 온 탓이다. 이에 오는 2030년까지 2018년 대비 12%, 2050년까지 53%의 온실가스 감축을 목표로 설정하고 있다. 이를 위해 석회석 대신 비탄산염 원료를 활용한 저탄소 클링커 기술, 폐합성수지·바이오매스 등 순환자원 연료 전환 기술 개발 등에 집중하고 있다.\n",
    "\n",
    "한일시멘트의 이번 결정은 불황기의 생존 해법으로 지배구조 단순화 및 비용 최적화의 가능성을 제기했다는 분석이 나온다.\n",
    "\n",
    "업계 관계자는 \"내수 의존도가 큰 기업일수록 버틸 수 있는 시간이 길지 않다\"며 \"누가 먼저 구조를 바꾸고, 얼마나 오래버티느냐가 관건\"이라고 말했다.\n",
    "    \"\"\",\n",
    "    \n",
    "    # 3. 증권 카테고리\n",
    "    \"\"\" (서울=연합뉴스) 임화섭 기자 = 인공지능(AI) 분야 경쟁으로 '매그니피센트 세븐'(미국의 7대 테크 대기업·The Magnificent Seven) 사이에서도 주가 차별화가 심화되고 있다고 미국 일간 월스트리트저널(WSJ)이 20일(현지시간) 분석했다.\n",
    "\n",
    "테크업계를 주름잡는 대기업들이며 각종 주가지수에서도 큰 비중을 차지하는 아마존닷컴(이하 아마존), 알파벳, 애플, 메타플랫폼스(이하 메타), 마이크로소프트(이하 MS), 엔비디아, 테슬라 등 7개 기업의 주가 흐름이 AI 분야 성과에 따라 크게 갈리는 경향이 최근 뚜렷해졌다는 것이다.\n",
    "\n",
    "대표적인 주가지수인 S&P 500에서 이들 7개 업체가 차지하는 비중은 35%에 이른다.\n",
    "\n",
    "최근 모건스탠리 보고서에 따르면 이 업체들의 2분기 순이익은 전년 동기 대비 14% 증가할 것으로 전망되며, 이는 나머지 493개 S&P 500 기업의 순이익이 전년 동기 대비 3% 감소할 것으로 예상되는 전반적 증시 분위기와 비교하면 훨씬 좋다.\n",
    "\n",
    "이들 중 알파벳을 제외한 6개 업체들은 최근 주가가 향후 1년간 예상 순이익의 25배를 넘었으며, 이는 최근 S&P 500 기업의 평균 주가수익비율(PER) 22.35보다 높다.\n",
    "\n",
    "올해 들어 엔비디아, 메타, MS의 주가는 약 20% 이상 올랐으나, 이와 대조적으로 애플은 16% 하락했고 알파벳도 2% 내렸고 테슬라는 18%가 빠졌다.\n",
    "\n",
    "AI 스타트업 '앤스로픽'에 투자한 아마존은 관세 등의 영향으로 전자상거래 부문 사업에 타격이 상당히 큰 가운데서도 3% 올랐다.\n",
    "\n",
    "알파벳과 테슬라는 오는 23일에, 메타, MS, 애플은 그 다음 주에 분기 실적을 발표할 예정이다.\n",
    "\n",
    "해리스 파이낸셜 그룹의 매니징 파트너인 제이미 콕스는 WSJ에 \"(테크 대기업들 사이에서 실적에 따라 주가 흐름이 차별화되는 것은) 불가피한 일이었다. 다른 일들을 하고 있기 때문에 영원히 서로 똑같은 (주가) 흐름이 계속되는 것은 불가능하다\"며 \"이제 승자와 패자의 계층분화가 왔다\"고 평가했다.\n",
    "\n",
    "2023년에 이 7개 업체를 묶어서 '매그니피센트 세븐'이라는 표현을 쓰기 시작한 뱅크오브아메리카 글로벌 리서치의 투자전략담당 책임자 마이클 하트넛은 이들 업체를 묶어서 본 중요한 이유 중 하나는 이들이 AI 분야의 선봉에 있기 때문이라고 말한 바 있다.\n",
    "\n",
    "그러나 투자회사 스피어의 창립자 겸 투자책임자인 이바나 델레브스카는 \"지금은 펀더멘털에서 (7개 업체들 사이에) 상당히 큰 차이가 보이고 있다\"고 말했다.\n",
    "    \"\"\"\n",
    "]\n",
    "def generate_output(model, text, system_prompt):\n",
    "    # messages 형식으로 프롬프트 구성 (system + user)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"다음 뉴스 텍스트를 분석해주세요:\\n\\n{text}\"}\n",
    "    ]\n",
    "    \n",
    "    # apply_chat_template 사용 (thinking 끄기: enable_thinking=False)\n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False  # thinking 모드 OFF (여기서 변경!)\n",
    "    )\n",
    "    \n",
    "    # 입력 토큰화\n",
    "    inputs = tokenizer([prompt_text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # generate (deterministic으로 설정)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_length=8192,  # 최대 길이\n",
    "            temperature=0.5,  # deterministic (랜덤성 제거)\n",
    "            top_p=1.0,\n",
    "            do_sample=False   # 샘플링 off\n",
    "        )\n",
    "    \n",
    "    # 출력 디코딩 (thinking이 없을 테니 전체 출력)\n",
    "    output_ids = generated_ids[0][len(inputs.input_ids[0]):]  # 생성된 부분만\n",
    "    decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "    \n",
    "    # 만약 thinking 태그가 남아 있으면 파싱 (안전장치, False로 하면 불필요)\n",
    "    try:\n",
    "        # </think> 토큰 ID (Qwen 공식: 151668, 모델에 따라 확인 필요)\n",
    "        think_end_token = 151668  # Qwen의 </think> 토큰 ID\n",
    "        index = len(output_ids.tolist()) - output_ids.tolist()[::-1].index(think_end_token)\n",
    "    except ValueError:\n",
    "        index = 0  # thinking 없으면 0\n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip()\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip()\n",
    "    \n",
    "    # thinking이 있으면 출력, 없으면 content만\n",
    "    if thinking_content:\n",
    "        print(\"주의: thinking content가 남아 있음:\", thinking_content)\n",
    "    \n",
    "    return content  # 최종 content 반환 (dictionary 예상)\n",
    "\n",
    "# 5. 3개 데이터로 베이스 모델 generate 실행\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    # print(f\"\\n===== 테스트 데이터 {i} (카테고리 힌트: 부동산/산업/증권) =====\\n{text}\\n\")\n",
    "    \n",
    "    # generate 호출 (모델 로드 후 바로 여기서 generate)\n",
    "    base_output = generate_output(base, text, system_prompt)\n",
    "    print(\"----- 베이스 모델 출력 -----\\n\", base_output)\n",
    "    \n",
    "    # generate 호출 (모델 로드 후 바로 여기서 generate)\n",
    "    base_output = generate_output(lora, text, system_prompt)\n",
    "    print(\"----- 로라 모델 출력 -----\\n\", base_output)\n",
    "    \n",
    "    print(f\"\\n===== {i}번째 비교 테스트 끝 =====\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13dbf92f-5a54-463a-b668-fbb40c8e2233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1-norm 합: 24584.0718\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "import torch\n",
    "\n",
    "path = \"./qwen3-lora-ft-final/adapter_model.safetensors\"\n",
    "state = load_file(path)                 # dict[str, torch.Tensor]\n",
    "\n",
    "# LoRA 파라미터만 골라서 L1-norm 합계 구하기\n",
    "total = sum(t.abs().sum().item()\n",
    "            for k, t in state.items()\n",
    "            if \"lora_\" in k)\n",
    "\n",
    "print(f\"L1-norm 합: {total:.4f}\")\n",
    "# 0 이면 학습이 안 된 것, 0보다 크면 정상 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b27f4b-60c7-4fa1-8b4b-b04b46b67aa3",
   "metadata": {},
   "source": [
    "# 최종 배치 데이터 확인해보기 (배치 1로 변경) (collator 잘 됐는지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b0475e5-37fe-40cf-8b9c-e878a2183cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06290bbb991e4342aecc877ee874dcb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/3265 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "test_training_args = SFTConfig(\n",
    "    output_dir=\"./qwen3-lora-ft\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    optim=\"adamw_torch\",\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    logging_steps=50,\n",
    "    save_steps=50,\n",
    "    save_total_limit=3,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    weight_decay=0.01,\n",
    "    max_seq_length = 8192,\n",
    "    packing        = False,\n",
    ")\n",
    "\n",
    "\n",
    "# 5) SFTTrainer\n",
    "test_trainer = SFTTrainer(\n",
    "    model             = model,\n",
    "    args              = test_training_args,           # ← SFTConfig 객체\n",
    "    train_dataset     = tokenized_dataset,\n",
    "    eval_dataset      = None,\n",
    "    processing_class  = tokenizer,         # ← tokenizer 자리\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a95167d4-d794-4e38-b802-8af2fcce2abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d873577a-6602-4a09-937a-740304faec7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value('string'),\n",
       " 'input_ids': List(Value('int32')),\n",
       " 'attention_mask': List(Value('int8')),\n",
       " 'labels': List(Value('int64'))}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e2f8dbe-e43d-4451-8741-1a4c3e222d22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 첫 번째 배치 확인 =====\n",
      "Batch keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "INPUT_IDS:\n",
      "  Shape: torch.Size([2, 1378])\n",
      "  Sample (첫 20개 토큰): [151644, 8948, 198, 64795, 82528, 33704, 5140, 231, 112, 24897, 10764, 44104, 53189, 18411, 128618, 129150, 82190, 90711, 112, 130229]\n",
      "  Sample (마지막 20개 토큰): [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643]\n",
      "\n",
      "ATTENTION_MASK:\n",
      "  Shape: torch.Size([2, 1378])\n",
      "  Sample (첫 20개 토큰): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Sample (마지막 20개 토큰): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "LABELS:\n",
      "  Shape: torch.Size([2, 1378])\n",
      "  Sample (첫 20개 토큰): [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "  Sample (마지막 20개 토큰): [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      " Labels에서 학습되는 토큰 수 (-100 아닌 부분): 162\n"
     ]
    }
   ],
   "source": [
    "# (이미 trainer가 정의된 후) dataloader 가져오기\n",
    "test_train_dataloader = test_trainer.get_train_dataloader()\n",
    "\n",
    "# 첫 번째 배치 가져오기 (작은 배치로 테스트)\n",
    "batch = next(iter(test_train_dataloader))  # dataloader의 첫 배치 추출\n",
    "\n",
    "# 배치 내용 확인 (input_ids, attention_mask, labels 출력)\n",
    "print(\"===== 첫 번째 배치 확인 =====\")\n",
    "print(f\"Batch keys: {batch.keys()}\")  # 예상: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# 각 키의 shape와 샘플 값 출력\n",
    "for key in ['input_ids', 'attention_mask', 'labels']:\n",
    "    print(f\"\\n{key.upper()}:\")\n",
    "    print(f\"  Shape: {batch[key].shape}\")  # 예: torch.Size([batch_size, sequence_length])\n",
    "    print(f\"  Sample (첫 20개 토큰): {batch[key][0][:20].tolist()}\")  # 첫 샘플의 앞부분\n",
    "    print(f\"  Sample (마지막 20개 토큰): {batch[key][0][-20:].tolist()}\")  # 첫 샘플의 뒷부분 (패딩 확인)\n",
    "\n",
    "# labels가 제대로 설정됐는지 추가 체크 (assistant 부분만 학습되는지)\n",
    "# -100이 아닌 부분이 assistant 답변인지 수동으로 확인하세요!\n",
    "non_ignore_count = (batch['labels'] != -100).sum().item()\n",
    "print(f\"\\n Labels에서 학습되는 토큰 수 (-100 아닌 부분): {non_ignore_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4953b8d1-fbc1-4f17-a8fe-ae6d30ad5050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc6fa5a9-7620-4d19-99c2-8a6a4e017ea6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== labels[0] 전체 (또는 일부) =====\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 151667, 271, 151668, 271, 4913, 5471, 788, 330, 134066, 2198, 3087, 3180, 788, 220, 17, 1335, 12389, 788, 4383, 126407, 66845, 143995, 19969, 138373, 13935, 131226, 56039, 77002, 133196, 133627, 125466, 138279, 26699, 126429, 32831, 66019, 54070, 142713, 17877, 16235, 229, 129062, 52959, 129296, 41671, 31328, 13146, 10465, 330, 126407, 66845, 125625, 16560, 126440, 124785, 56475, 130847, 132920, 364, 144796, 40281, 29346, 10764, 236, 246, 24897, 131131, 132466, 73077, 131196, 79207, 44680, 242, 120, 29346, 6, 56475, 48364, 104, 126429, 32831, 66019, 56419, 20487, 125625, 31328, 364, 52959, 12802, 57268, 144310, 20, 451, 6, 17877, 125466, 59761, 51876, 1189, 13989, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "assistant 부분 (idx 881 ~ 982):\n",
      "input_ids: [151667, 271, 151668, 271, 4913, 5471, 788, 330, 134066, 2198, 3087, 3180, 788, 220, 17, 1335, 12389, 788, 4383, 126407, 66845, 143995, 19969, 138373, 13935, 131226, 56039, 77002, 133196, 133627, 125466, 138279, 26699, 126429, 32831, 66019, 54070, 142713, 17877, 16235, 229, 129062, 52959, 129296, 41671, 31328, 13146, 10465, 330, 126407, 66845, 125625, 16560, 126440, 124785, 56475, 130847, 132920, 364, 144796, 40281, 29346, 10764, 236, 246, 24897, 131131, 132466, 73077, 131196, 79207, 44680, 242, 120, 29346, 6, 56475, 48364, 104, 126429, 32831, 66019, 56419, 20487, 125625, 31328, 364, 52959, 12802, 57268, 144310, 20, 451, 6, 17877, 125466, 59761, 51876, 1189, 13989, 151645]\n",
      "labels: [151667, 271, 151668, 271, 4913, 5471, 788, 330, 134066, 2198, 3087, 3180, 788, 220, 17, 1335, 12389, 788, 4383, 126407, 66845, 143995, 19969, 138373, 13935, 131226, 56039, 77002, 133196, 133627, 125466, 138279, 26699, 126429, 32831, 66019, 54070, 142713, 17877, 16235, 229, 129062, 52959, 129296, 41671, 31328, 13146, 10465, 330, 126407, 66845, 125625, 16560, 126440, 124785, 56475, 130847, 132920, 364, 144796, 40281, 29346, 10764, 236, 246, 24897, 131131, 132466, 73077, 131196, 79207, 44680, 242, 120, 29346, 6, 56475, 48364, 104, 126429, 32831, 66019, 56419, 20487, 125625, 31328, 364, 52959, 12802, 57268, 144310, 20, 451, 6, 17877, 125466, 59761, 51876, 1189, 13989, -100]\n",
      "같은가? : False\n",
      "assistant 부분 학습 토큰 수: 100\n"
     ]
    }
   ],
   "source": [
    "# batch 이미 가져온 후 (trainer.get_train_dataloader() 후 next(iter()))\n",
    "import torch\n",
    "\n",
    "# labels[0] 전체 출력 (너무 길면 앞뒤 자름, but 전체 확인 추천)\n",
    "print(\"===== labels[0] 전체 (또는 일부) =====\")\n",
    "print(batch['labels'][0].tolist())  # 전체 리스트로 출력\n",
    "\n",
    "# assistant 시작 위치 찾기 (Qwen 토큰 기준, <im_start|>assistant\\n)\n",
    "assistant_start_tokens = [151644, 77091, 198]  # <im_start|>assistant\\n 토큰 (tokenizer 확인 필요)\n",
    "input_ids_list = batch['input_ids'][0].tolist()\n",
    "labels_list = batch['labels'][0].tolist()\n",
    "\n",
    "start_idx = -1\n",
    "for i in range(len(input_ids_list) - len(assistant_start_tokens) + 1):\n",
    "    if input_ids_list[i:i+len(assistant_start_tokens)] == assistant_start_tokens:\n",
    "        start_idx = i + len(assistant_start_tokens)  # assistant\\n 다음부터\n",
    "        break\n",
    "\n",
    "if start_idx != -1:\n",
    "    # assistant 끝 찾기 (<|im_end|>: 151645)\n",
    "    end_idx = start_idx\n",
    "    for j in range(start_idx, len(input_ids_list)):\n",
    "        if input_ids_list[j] == 151645:  # <|im_end|>\n",
    "            end_idx = j + 1  # include end token\n",
    "            break\n",
    "    else:\n",
    "        end_idx = len(input_ids_list)  # end 없으면 전체\n",
    "\n",
    "    print(f\"\\nassistant 부분 (idx {start_idx} ~ {end_idx}):\")\n",
    "    print(\"input_ids:\", input_ids_list[start_idx:end_idx])\n",
    "    print(\"labels:\", labels_list[start_idx:end_idx])\n",
    "    print(\"같은가? :\", input_ids_list[start_idx:end_idx] == labels_list[start_idx:end_idx]) \n",
    "\n",
    "    # -100 아닌 개수 다시 체크\n",
    "    assistant_non_ignore = sum(1 for x in labels_list[start_idx:end_idx] if x != -100)\n",
    "    print(f\"assistant 부분 학습 토큰 수: {assistant_non_ignore}\")  # ~81 정도 예상 (배치당)\n",
    "else:\n",
    "    print(\"assistant 시작 못 찾음 - 데이터셋 문제?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9318691-f7c6-4f61-a21a-7246cd549a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "당신은 뉴스 텍스트를 분석하여 카테고리를 분류하고 주요 핵심 이벤트들을 추출하는 전문 분석 시스템입니다.\n",
      "주어진 텍스트를 분석하여 반드시 파이썬의 dictionary 형식으로 결과를 작성하십시오.\n",
      "큰 따옴표 사이에 다른 따옴표들을 적으려고 시도하지 마십시오. 이는 dictionary 파싱을 실패하게 하는 원인이 됩니다.\n",
      "아래 dictionary에서 각 value는 지시사항에 해당합니다. 지시사항을 그대로 적지 마시고, 해당 지시사항에 따라 적절한 value를 채워넣으십시오.\n",
      "\n",
      "분석 결과는 다음 형식으로 작성하십시오:\n",
      "\n",
      "답변:\n",
      "{\"category\": \"텍스트의 카테고리를 ['부동산', '산업', '오피니언', '증권'] 중 하나로 분류하여 작성하십시오\",\n",
      "\"event_count\": \"텍스트 내에서 발견된 핵심 이벤트의 개수를 정수로 작성하십시오(증감, 변화, 변동이 나오는 구절을 중점적으로 보세요.)\",\n",
      "\"events\": [\"텍스트에서 추출한 각 핵심 이벤트를 요약하는 문장들을 파이썬 문자열 리스트 형태로 작성하십시오. 원문의 핵심 내용을 그대로 유지하되 한 문장으로 간결하게 작성하십시오\"]}<|im_end|>\n",
      "<|im_start|>user\n",
      "다음 뉴스 텍스트를 분석해주세요:\n",
      "\n",
      "현대자동차가 중국·북미 등 세계시장 공략용 고성능 모델을 잇달아 선보인다. 고급화·전동화에 이어 고성능화에 속도를 냄으로써 브랜드 이미지를 더욱 끌어올리겠다는 전략으로 풀이된다.  3일 자동차 업계에 따르면 현대차는 고성능 N 브랜드 신차인 '더 뉴 아반떼 N(현지명 더 뉴 엘란트라 N)'을 오는 10월 중국, 11월 북미 지역에 잇따라 출시할 계획이다. 한국시장에 내놓는 건 10월이 될 전망이다. 아반떼 N은 지난 3월 출시한 부분변경 차량인 '더 뉴 아반떼'의 고성능 세단 모델이다. 현대차는 지난 4월 열린 상하이 국제모터쇼에서 N 브랜드의 중국시장 진출을 공식 선언한 바 있다. 세계 최대 자동차시장인 중국에서 점유율이 1.7%(작년 기준)에 불과할 정도로 부진을 겪는 현대차의 새로운 승부수다. 현대차는 중국 내 기존 판매 라인업을 현재 13종에서 8종으로 줄이는 대신, N 브랜드를 상하이를 중심으로 판매한다는 전략이다.  2021년 북미에 아반떼 N을 선보였던 현대차는 새 모델도 연말께 현지에 내놓으며 도전을 이어간다. 일반 모델 아반떼는 미국에서 가장 많이 팔린 현대차 차량이기도 하다. 이달에는 N 브랜드 전기차도 탄생한다. 현대차는 영국에서 열리는 '굿우드 페스티벌 오브 스피드'에서 첫 고성능 전기차인 '아이오닉5 N'을 공개한다. 현대차는 연내 한국·미국·중국 등에 아이오닉5 N을 차례로 선보이고, 내년 초 일본 공략에도 나설 계획이다.   [이유섭 기자]<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "{\"category\": \"산업\",\"event_count\": 2,\"events\": [\"현대자동차가 중국·북미 등 세계시장 공략용 고성능 모델을 잇달아 선보인다.\", \"현대차는 영국에서 열리는 '굿우드 페스티벌 오브 스피드'에서 첫 고성능 전기차인 '아이오닉5 N'을 공개한다.\"]}<|im_end|>\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = test_trainer.tokenizer           # SFTTrainer 가 자동으로 갖고 있습니다\n",
    "\n",
    "ids = batch[\"input_ids\"][0].tolist()         \n",
    "txt = tokenizer.decode(ids, skip_special_tokens=False)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4588f09d-b9ff-400d-9ffc-b95be4b50ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids.shape : torch.Size([2, 1378])\n",
      "real lengths    : [983, 1378]\n",
      "pad lengths     : [395, 0]\n",
      "\n",
      "----- raw ids 끝 20개 -----\n",
      "tensor([151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643], device='cuda:0')\n",
      "\n",
      "----- attention_mask 끝 20개 -----\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "\n",
      "실제 토큰 수 : 983\n",
      "system\n",
      "당신은 뉴스 텍스트를 분석하여 카테고리를 분류하고 주요 핵심 이벤트들을 추출하는 전문 분석 시스템입니다.\n",
      "주어진 텍스트를 분석하여 반드시 파이썬의 dictionary 형식으로 결과를 작성하십시오.\n",
      "큰 따옴표 사이에 다른 따옴표들을 적으려고 시도하지 마십시오. 이는 dictionary 파싱을 실패하게 하는 원인이 됩니다.\n",
      "아래 dictionary에서 각 value는 지시사항에 해당합니다. 지시사항을 그대로 적지 마시고, 해당 지시사항에 따라 적절한 value를 채워넣으십시오.\n",
      "\n",
      "분석 결과는 다음 형식으로 작성하십시오:\n",
      "\n",
      "답변:\n",
      "{\"category\": \"텍스트의 카테고리를 ['부동산', '산업', '오피니언', '증권'] 중 하나로 분류하여 작성하십시오\",\n",
      "\"event_count\": \"텍스트 내에서 발견된 핵심 이벤트의 개수를 정수로 작성하십시오(증감, 변화, 변동이 나오는 구절을 중점적으로 보세요.)\",\n",
      "\"events\": [\"텍스트에서 추출한 각 핵심 이벤트를 요약하는 문장들을 파이썬 문자열 리스트 형태로 작성하십시오. 원문의 핵심 내용을 그대로 유지하되 한 문장으로 간결하게 작성하십시오\"]}\n",
      "user\n",
      "다음 뉴스 텍스트를 분석해주세요:\n",
      "\n",
      "현대자동차가 중국·북미 등 세계시장 공략용 고성능 모델을 잇달아 선보인다. 고급화·전동화에 이어 고성능화에 속도를 냄으로써 브랜드 이미지를 더욱 끌어올리겠다는 전략으로 풀이된다.  3일 자동차 업계에 따르면 현대차는 고성능 N 브랜드 신차인 '더 뉴 아반떼 N(현지명 더 뉴 엘란트라 N)'을 오는 10월 중국, 11월 북미 지역에 잇따라 출시할 계획이다. 한국시장에 …\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dl = test_trainer.get_train_dataloader()\n",
    "batch = next(iter(dl))              # 아무 배치 하나 뽑기\n",
    "\n",
    "ids        = batch[\"input_ids\"]         # (B, L)\n",
    "attn_mask  = batch[\"attention_mask\"]    # (B, L)\n",
    "\n",
    "# ① 배치 전체 정보\n",
    "print(\"input_ids.shape :\", ids.shape)\n",
    "print(\"real lengths    :\", attn_mask.sum(1).tolist())\n",
    "print(\"pad lengths     :\", (ids.shape[1] - attn_mask.sum(1)).tolist())\n",
    "\n",
    "# ② 패딩이 진짜 0 인지, 특수 토큰은 제외됐는지\n",
    "row = 0                        # 보고 싶은 시퀀스 인덱스\n",
    "print(\"\\n----- raw ids 끝 20개 -----\")\n",
    "print(ids[row, -20:])\n",
    "\n",
    "print(\"\\n----- attention_mask 끝 20개 -----\")\n",
    "print(attn_mask[row, -20:])\n",
    "\n",
    "# ③ attention_mask==1 위치만 디코딩\n",
    "tok = test_trainer.processing_class   # Trainer.tokenizer 대신\n",
    "real_ids = ids[row][attn_mask[row].bool()]\n",
    "print(f\"\\n실제 토큰 수 : {real_ids.size(0)}\")\n",
    "\n",
    "text = tok.decode(real_ids, skip_special_tokens=True)\n",
    "print(text[:800] + (\" …\" if len(text) > 800 else \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b78f0f66-2324-4bdf-909d-bae7a2a135ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids.shape : torch.Size([2, 1378])\n",
      "최대 길이        : 1378\n",
      "배치별 실제 길이 : [983, 1378]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dl = test_trainer.get_train_dataloader()\n",
    "batch = next(iter(dl))\n",
    "\n",
    "print(\"input_ids.shape :\", batch[\"input_ids\"].shape)      # (batch, seq_len)\n",
    "print(\"최대 길이        :\", batch[\"attention_mask\"].sum(1).max().item())\n",
    "print(\"배치별 실제 길이 :\", batch[\"attention_mask\"].sum(1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c11d1b16-d0b1-46c7-829a-670978528bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "─── 샘플 0 / 실제 길이 983 ───\n",
      "system\n",
      "당신은 뉴스 텍스트를 분석하여 카테고리를 분류하고 주요 핵심 이벤트들을 추출하는 전문 분석 시스템입니다.\n",
      "주어진 텍스트를 분석하여 반드시 파이썬의 dictionary 형식으로 결과를 작성하십시오.\n",
      "큰 따옴표 사이에 다른 따옴표들을 적으려고 시도하지 마십시오. 이는 dictionary 파싱을 실패하게 하는 원인이 됩니다.\n",
      "아래 dictionary에서 각 value는 지시사항에 해당합니다. 지시사항을 그대로 적지 마시고, 해당 지시사항에 따라 적절한 value를 채워넣으십시오.\n",
      "\n",
      "분석 결과는 다음 형식으로 작성하십시오:\n",
      "\n",
      "답변:\n",
      "{\"category\": \"텍스트의 카테고리를 ['부동산', '산업', '오피니언', '증권'] 중 하나로 분류하여 작성하십시오\",\n",
      "\"event_count\": \"텍스트 내에서 발견된 핵심 이벤트의 개수를 정수로 작성하십시오(증감, 변화, 변동이 나오는 구절을 중점적으로 보세요.)\",\n",
      "\"events\": [\"텍스트에서 추출한 각 핵심 이벤트를 요약하는 문장들을 파이썬 문자열 리스트 형태로 작성하십시오. 원문의 핵심 내용을 그대로 유지하되 한 문장으로 간결하게 작성하십시오\"]}\n",
      "user\n",
      "다음 뉴스 텍스트를 분석해주세요:\n",
      "\n",
      "현대자동차가 중국·북미 등 세계시장 공략용 고성능 모델을 잇달아 선보인다. 고급화·전동화에 이어 고성능화에 속도를 냄으로써 브랜드 이미지를 더욱 끌어올리겠다는 전략으로 풀이된다.  3일 자동차 업계에 따르면 현대차는 고성능 N 브랜드 신차인 '더 뉴 아반떼 N(현지명 더 뉴 엘란트라 N)'을 오는 10월 중국, 11월 북미 지역에 잇따라 출시할 계획이다. 한국시장에 내놓는 건 10월이 될 전망이다. 아반떼 N은 지난 3월 출시한 부분변경 차량인 '더 뉴 아반떼'의 고성능 세단 모델이다. 현대차는 지난 4월 열린 상하이 국제모터쇼에서 N 브랜드의 중국시장 진출을 공식 선언한 바 있다. 세계 최대 자동차시장인 중국에서 점유율이 1.7%(작년 기준)에 불과할 정도로 부진을 겪는 현대차의 새로운 승부수다. 현대차는 중국 내 기존 판매 라인업을 현재 13종에서 8종으로 줄이는 대신, N 브랜드를 상하이를 중심으로 판매한다는 전략이다.  2021년 북미에 아반떼 N을 선보였던 현대차는 새 모델도 연말께 현지에 내놓으며 도전을 이어간다. 일반 모델 아반떼는 미국에서 가장 많이 팔린 현대차 차량이기도 하다. 이달에는 N 브랜드 전기차도 탄생한다. 현대차는 영국에서 열리는 '굿우드 페스티벌 오브 스피드'에서 첫 고성능 전기차인 '아이오닉5 N'을 공개한다. 현대차는 연내 한국·미국·중국 등에 아이오닉5 N을 차례로 선보이고, 내년 초 일본 공략에도 나설 계획이다.   [이유섭 기자]\n",
      "assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "{\"category\": \"산업\",\"event_count\": 2,\"events\": [\"현대자동차가 중국·북미 등 세계시장 공략용 고성능 모델을 잇달아 선보인다.\", \"현대차는 영국에서 열리는 '굿우드 페스티벌 오브 스피드'에서 첫 고성능 전기차인 '아이오닉5 N'을 공개한다.\"]}\n",
      "\n",
      "\n",
      "─── 샘플 1 / 실제 길이 1378 ───\n",
      "system\n",
      "당신은 뉴스 텍스트를 분석하여 카테고리를 분류하고 주요 핵심 이벤트들을 추출하는 전문 분석 시스템입니다.\n",
      "주어진 텍스트를 분석하여 반드시 파이썬의 dictionary 형식으로 결과를 작성하십시오.\n",
      "큰 따옴표 사이에 다른 따옴표들을 적으려고 시도하지 마십시오. 이는 dictionary 파싱을 실패하게 하는 원인이 됩니다.\n",
      "아래 dictionary에서 각 value는 지시사항에 해당합니다. 지시사항을 그대로 적지 마시고, 해당 지시사항에 따라 적절한 value를 채워넣으십시오.\n",
      "\n",
      "분석 결과는 다음 형식으로 작성하십시오:\n",
      "\n",
      "답변:\n",
      "{\"category\": \"텍스트의 카테고리를 ['부동산', '산업', '오피니언', '증권'] 중 하나로 분류하여 작성하십시오\",\n",
      "\"event_count\": \"텍스트 내에서 발견된 핵심 이벤트의 개수를 정수로 작성하십시오(증감, 변화, 변동이 나오는 구절을 중점적으로 보세요.)\",\n",
      "\"events\": [\"텍스트에서 추출한 각 핵심 이벤트를 요약하는 문장들을 파이썬 문자열 리스트 형태로 작성하십시오. 원문의 핵심 내용을 그대로 유지하되 한 문장으로 간결하게 작성하십시오\"]}\n",
      "user\n",
      "다음 뉴스 텍스트를 분석해주세요:\n",
      "\n",
      "한국토지주택공사(LH)가 올해 전국에서 공공분양 아파트를 6000여 가구 공급할 예정이다. 이중 80% 는 수도권에 몰려 있어 관심이 집중된다.  5일 LH는 전국에서 분양주택 6353가구와 임대주택 6만8223가구 등 총 7만4576가구에 대한 입주자모집을 실시할 계획이라고 밝혔다. 우선 분양주택은 전국 14개 단지에서 공급된다. 윤석열 정부가 새롭게 선보인 공공분양 모델인 뉴홈의 일반형으로 5개 단지 3165가구가 공급되고, 나머지 9개 단지 3188가구는 신혼희망타운으로 나온다. 특히 뉴홈 일반형의 경우 4050세대 등 무주택 장년층에게 유리한 일반공급 물량이 기준 15%에서 30%로 확대된 점이 특징이다. 일반공급 물량의 20%는 추첨 방식으로 공급된다.  수도권 물량이12개 단지 5181가구다. 가구수 기준으로 전국 공급량의 81.6%에 해당된다. 이중에는 서울대방1(122가구·4월 공급예정)과 서울공릉1(154가구·10월 예정) 등 서울 지역 공공분양 아파트도 포함돼있다.  올해 LH 공공분양은 서울 외에도 입지적으로 눈여겨볼 만한 단지들이 여럿 있다. 위례신도시에 있는 A2-7블록에서 440가구(6월)가 신혼희망타운으로 공급되고, 강남과 판교 사이에 위치한 성남신촌지구에선 320가구(A2블록)가 뉴홈 일반형으로 오는 7월 나온다. 3기 신도시 중에선 인천계양 A2블록(747가구)이 뉴홈 일반형으로, A3블록(359가구)은 신혼희망타운으로 각각 10월 공급된다.  이밖에도 △6월 파주운정3 A22블록(642가구), 화성태안3 B-3블록(688가구·이상 뉴홈 일반형) △8월 고양장한 A-2블록(371가구·신혼희망타운) △12월 인천가정2 A2블록), 수원당수 A5블록(600가구), 의왕청계2 A1블록(320가구·이상 신혼희망타운) 등이 수도권에서 공급될 예정이다. 지방에선 10월에 부산문현2 01블록(768가구), 12월엔 남원주역세권 A-3블록(404가구) 등 2개 단지가 청약을 기다리고 있다. 뉴홈과 신혼희망타운 모두 분양가 상한제가 적용된다.  주변 임대료 대비 30~80% 수준으로 저렴하게 공급되는 공공임대주택은 올해 전국서 총 6만7000가구 신규 공급된다. 유형별로는 △건설임대 1만1683가구 △매입임대 2만6380가구 △전세임대 3만160가구 등이다. 주요 건설임대 단지로는 성남신흥2(812가구·4월), 성남복정1(234가구), 강동천호(94가구), 서울대방(61가구·이상 6월) 등이 예정돼있다.\n",
      "assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "{\"category\": \"부동산\",\"event_count\": 2,\"events\": [\"뉴홈과 신혼희망타운 모두 분양가 상한제가 적용된다.\", \"우선 분양주택은 전국 14개 단지에서 공급된다.\"]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tokenizer = test_trainer.tokenizer      # 이미 로드돼 있음\n",
    "batch     = next(iter(test_trainer.get_train_dataloader()))\n",
    "\n",
    "for i in range(batch[\"input_ids\"].size(0)):          # 배치(=2개 샘플) 순회\n",
    "    # 1) attention_mask==1 인 부분만 잘라서 실제 토큰만 남김\n",
    "    real_ids = batch[\"input_ids\"][i][batch[\"attention_mask\"][i].bool()]\n",
    "\n",
    "    # 2) 디코딩\n",
    "    text = tokenizer.decode(real_ids, skip_special_tokens=True)\n",
    "\n",
    "    # 3) 결과 출력\n",
    "    print(f\"\\n─── 샘플 {i} / 실제 길이 {real_ids.size(0)} ───\")\n",
    "    print(text[:8192] + (\" …\" if len(text) > 8192 else \"\"))   # 너무 길면 앞 800자만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e88426-d536-48ad-981f-9692d079df34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
